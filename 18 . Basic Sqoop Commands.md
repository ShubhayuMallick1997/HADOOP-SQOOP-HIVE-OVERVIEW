# 18 . **Basic Sqoop Commands**

Here are the essential **basic Sqoop commands** that are used to import data from relational databases (RDBMS) into Hadoop (HDFS, Hive, HBase) and export data from Hadoop to RDBMS. These commands facilitate seamless integration between the relational and Hadoop worlds.

---

## **1. Import Data from RDBMS to HDFS**

**The `sqoop import` command** is used to import data from a relational database into HDFS. This command can be customized based on various options like splitting data, specifying the target directory in HDFS, and handling large data sets.

## **Basic Import Command**:

The basic syntax for importing data from an RDBMS (like MySQL) into HDFS is:

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username <username> --password <password> --table <table_name> --target-dir <hdfs_target_dir>
```

* **`--connect`**: JDBC URL of the relational database.
* **`--username`**: Username for the database.
* **`--password`**: Password for the database.
* **`--table`**: The name of the table to be imported.
* **`--target-dir`**: The HDFS directory where the imported data will be stored.

## **Example**:

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --target-dir /user/hadoop/employees
```

This will import the data from the `employees` table in MySQL and store it in HDFS at `/user/hadoop/employees`.

---

## **2. Export Data from HDFS to RDBMS**

**The `sqoop export` command** is used to export data from HDFS back into a relational database. The source of the data in this case is a file or directory in HDFS, and the target is a table in a relational database.

## **Basic Export Command**:

The basic syntax for exporting data from HDFS to a relational database is:

```bash
sqoop export --connect jdbc:mysql://localhost/mydb --username <username> --password <password> --table <table_name> --export-dir <hdfs_input_dir>
```

* **`--connect`**: JDBC URL of the relational database.
* **`--username`**: Username for the database.
* **`--password`**: Password for the database.
* **`--table`**: The target table in the relational database.
* **`--export-dir`**: The HDFS directory containing the data to be exported.

## **Example**:

```bash
sqoop export --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --export-dir /user/hadoop/employees
```

This command exports the data from HDFS (`/user/hadoop/employees`) into the `employees` table in MySQL.

---

## **3. Importing with Different File Formats (Text, Avro, Parquet)**

Sqoop supports importing data from relational databases into HDFS in various file formats, such as **Text**, **Avro**, and **Parquet**. The choice of file format can impact storage efficiency, read performance, and processing speed.

## **Text File Format** (Default):

By default, Sqoop imports data in **text** format (CSV or tab-delimited). If no specific format is mentioned, this is the format used.

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --target-dir /user/hadoop/employees --as-textfile
```

## **Avro Format**:

**Avro** is a compact, fast, binary data serialization format. It is widely used in Hadoop for high-speed data processing and storage. Avro is ideal for cases where data schema evolution is needed.

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --target-dir /user/hadoop/employees --as-avrodatafile
```

This will import data from the `employees` table into HDFS in **Avro** format, which is more efficient for read-heavy operations.

## **Parquet Format**:

**Parquet** is a columnar storage format that provides efficient data compression and encoding schemes. It is commonly used for big data processing frameworks (such as Apache Spark) due to its fast read performance and high compression ratio.

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --target-dir /user/hadoop/employees --as-parquetfile
```

This will import the `employees` table into HDFS in **Parquet** format, suitable for analytics workloads.

---

## **4. Handling Data Types between SQL and HDFS**

When importing data from relational databases to HDFS using Sqoop, it’s crucial to understand how SQL data types map to HDFS data formats. Sqoop automatically maps most data types, but there are cases where manual mapping may be necessary.

## **Default Type Mappings**:

* **Integer/Long** → `int` (in HDFS or Avro)
* **String/Varchar** → `string` (in HDFS or Avro)
* **Date/Time** → `timestamp` (in HDFS or Avro)
* **Decimal/Float/Double** → `double` (in HDFS or Avro)
* **Boolean** → `boolean` (in HDFS or Avro)

## **Custom Type Mapping**:

In some cases, you might need to manually map SQL data types to HDFS data types to ensure compatibility. You can use the `--map-column-hive` option to specify how a column should be mapped to a Hive data type.

For example, if the `price` column in the database is of type `DECIMAL` but you want it to be imported as `FLOAT` in Hive, you can use the following command:

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table products --target-dir /user/hadoop/products --map-column-hive price=FLOAT
```

This command tells Sqoop to map the `price` column from the `DECIMAL` type in MySQL to `FLOAT` in Hive.

## **Handling Null Values**:

By default, Sqoop handles **null values** by assigning them a specific placeholder (e.g., empty string for strings, `0` for numeric types). However, in some cases, you may need to configure the handling of nulls explicitly:

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --target-dir /user/hadoop/employees --null-string '\\N' --null-non-string '\\N'
```

This will ensure that `NULL` values are properly handled in the HDFS import.

## **Data Type Conversions for HBase**:

When importing data into **HBase**, the **column family** and **column qualifier** need to be specified explicitly. HBase columns can be mapped to various data types, and Sqoop automatically handles conversions to **HBase types** like `byte[]`.

Example:

```bash
sqoop import --connect jdbc:mysql://localhost/mydb --username root --password mypassword --table employees --hbase-table employees --column-family info --hbase-row-key id
```

This imports data from the `employees` table into the `employees` HBase table, using the `id` column as the row key and placing the data under the `info` column family.

---

## **Conclusion**

The basic **Sqoop commands** for importing and exporting data between RDBMS and HDFS provide a flexible and efficient way to move large datasets. By supporting multiple file formats (Text, Avro, Parquet) and handling various data types, Sqoop offers a seamless integration of relational databases with Hadoop. The ability to handle complex data types and mappings ensures that data from different sources can be imported, processed, and exported with minimal overhead.

Let me know if you need further details on any of these commands or topics!
