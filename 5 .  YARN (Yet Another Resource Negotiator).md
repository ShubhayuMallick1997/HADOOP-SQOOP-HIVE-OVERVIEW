# **YARN (Yet Another Resource Negotiator)**

## **What is YARN?**

YARN, or **Yet Another Resource Negotiator**, is the resource management layer in the Hadoop ecosystem. It was introduced in Hadoop 2.x to address the limitations of the original MapReduce framework, where MapReduce acted both as the resource manager and the execution engine. YARN separates the resource management and job scheduling functionalities, which allows multiple applications to run simultaneously in a Hadoop cluster.

In essence, YARN manages resources across the cluster and schedules jobs to run on available nodes, enabling various frameworks (such as MapReduce, Apache Spark, Apache Tez) to run on the same cluster concurrently. It enhances the scalability and flexibility of Hadoop by enabling the execution of a variety of workloads.

## **YARN Architecture**

YARN introduces a two-layer architecture with the following components:

1. **ResourceManager (RM)**:

   * The **ResourceManager** is the master daemon responsible for managing the resources of the cluster. It performs two main functions:

     * **Resource Management**: It tracks available resources (memory, CPU) across all nodes in the cluster.
     * **Job Scheduling**: It assigns resources to different applications based on job priority and resource availability.

   The ResourceManager has two key sub-components:

   * **Scheduler**: The scheduler allocates resources to various running applications based on policies such as capacity, fairness, and priorities.
   * **ApplicationManager**: This component manages the lifecycle of applications, including accepting job submissions and negotiating resources with the ResourceManager.

2. **NodeManager (NM)**:

   * The **NodeManager** runs on each node in the cluster and manages resources on that node.
   * It monitors the resource usage (CPU, memory, disk) on the node and periodically sends resource usage reports to the ResourceManager.
   * The NodeManager also manages the execution of containers (which are isolated environments for running tasks), providing resources like memory and CPU, and ensuring tasks are properly executed on the node.

3. **ApplicationMaster (AM)**:

   * Each application that runs on YARN has its own **ApplicationMaster**.
   * The ApplicationMaster is responsible for negotiating resources with the ResourceManager and managing the lifecycle of tasks (i.e., tasks within MapReduce jobs or other frameworks like Spark).
   * It also monitors the progress of the job and handles task failures or retries.

4. **Containers**:

   * **Containers** are the basic units of execution in YARN. Each container encapsulates a specific amount of resources (e.g., memory, CPU cores) and runs a specific task.
   * The ResourceManager allocates containers to tasks via the NodeManager, which runs them on the node.

## **YARN Job Flow**

Here’s how a typical YARN job is executed:

1. **Job Submission**:

   * A client submits a job to YARN. The job can be a MapReduce, Spark, or any other supported framework job.

2. **Resource Request**:

   * The job’s **ApplicationMaster** communicates with the ResourceManager to request resources (containers) to run the job.

3. **Resource Allocation**:

   * The ResourceManager allocates resources (containers) based on available resources and scheduling policies, then provides these resources to the ApplicationMaster.

4. **Task Execution**:

   * The ApplicationMaster assigns tasks to containers and works with the NodeManagers to start execution.

5. **Task Monitoring**:

   * The NodeManager monitors the status of tasks in the containers and reports to the ResourceManager about the resource usage (CPU, memory).

6. **Completion**:

   * When the job completes, the ApplicationMaster reports back to the ResourceManager and the job is marked as successful or failed based on its progress.

## **ResourceManager Components**

1. **Scheduler**:

   * The Scheduler is responsible for allocating resources to applications based on resource requests. It doesn't track the status of tasks but ensures that applications get resources based on the policies defined.
   * There are different scheduling policies available, such as:

     * **Capacity Scheduler**: Guarantees a certain amount of resources to each application.
     * **Fair Scheduler**: Allocates resources to jobs in a way that all jobs get an equal share of resources over time.

2. **ApplicationManager**:

   * The **ApplicationManager** manages job lifecycle events and acts as the interface for handling the job submission process. It coordinates with the ResourceManager to acquire resources and starts the job in the containers.
   * After receiving the requested resources, the ApplicationMaster initiates tasks within the allocated containers and monitors their execution.

## **NodeManager Components**

1. **Container Management**:

   * The NodeManager’s responsibility is to manage containers, monitor resource usage (such as CPU, memory, disk), and restart tasks if needed.
   * It ensures that tasks are running within their allocated containers and reports resource usage back to the ResourceManager.

2. **Health Checks**:

   * The NodeManager also performs health checks for the tasks running on the node, ensuring that containers are not over-using resources or failing repeatedly.

## **YARN Resource Management Process**

1. **Job Request**: A job is submitted to the YARN ResourceManager.
2. **Container Allocation**: The ResourceManager allocates resources to the job (based on the Scheduler and policies).
3. **Task Execution**: The NodeManager allocates containers on specific nodes for running tasks.
4. **Progress Monitoring**: The NodeManager monitors resource usage and reports to the ResourceManager.
5. **Completion**: When the job finishes, the ApplicationMaster reports the status to the ResourceManager.

## **Advantages of YARN**

1. **Multi-Tenancy**:

   * YARN supports multi-tenancy, which means it allows multiple types of applications (MapReduce, Spark, Tez, etc.) to run on the same Hadoop cluster simultaneously, improving resource utilization and flexibility.

2. **Improved Scalability**:

   * By decoupling resource management from the application logic, YARN allows for better scalability. The ResourceManager can handle larger clusters and manage thousands of nodes and applications.

3. **Fault Tolerance**:

   * YARN’s architecture ensures that jobs can continue running even if individual nodes or applications fail. The ResourceManager and NodeManager handle failure recovery and task rescheduling.

4. **Resource Utilization**:

   * YARN improves resource utilization by managing CPU and memory resources effectively. It ensures that resources are not wasted, and applications use only the resources they need.

5. **Flexibility**:

   * YARN allows different types of data processing frameworks to coexist on the same cluster, making it a more versatile platform for diverse workloads.

## **YARN Configuration Parameters**

Here are some common configuration parameters for YARN:

1. **yarn.resourcemanager.scheduler.class**:

   * Specifies the type of scheduler to be used in YARN (e.g., CapacityScheduler or FairScheduler).

2. **yarn.nodemanager.resource.memory-mb**:

   * Specifies the total amount of memory (in MB) that the NodeManager can allocate to containers on that node.

3. **yarn.nodemanager.resource.cpu-vcores**:

   * Specifies the number of virtual CPU cores that can be allocated to containers on the node.

4. **yarn.scheduler.maximum-allocation-mb**:

   * Specifies the maximum memory allocation for any single container in YARN.

5. **yarn.am.resource.mb**:

   * Defines the memory allocated to the ApplicationMaster.

## **YARN Performance Tuning**

1. **Memory Allocation**:

   * Properly allocating memory to containers is critical for avoiding over-subscription and under-utilization. Set memory limits for containers, and ensure there’s enough memory for the NodeManager to manage the system.

2. **Scheduling Policies**:

   * Choosing the right scheduling policy (Capacity or Fair Scheduler) helps ensure fair resource allocation, especially in a multi-tenant environment with different priority applications.

3. **NodeManager Tuning**:

   * Proper tuning of NodeManager resource monitoring ensures the system is not overburdened. It’s essential to keep the resource usage within limits to avoid task failures and ensure stable performance.

## **Conclusion**

YARN is a crucial component of the Hadoop ecosystem, providing scalable and efficient resource management for a wide range of data processing applications. By separating resource management from job execution, it enables better resource utilization, job prioritization, and scalability. Whether you’re running MapReduce, Apache Spark, or other frameworks, YARN plays a vital role in ensuring the success of your Hadoop-based big data workflows.
