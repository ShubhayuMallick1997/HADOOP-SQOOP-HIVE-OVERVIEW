# **YARN (Yet Another Resource Negotiator)**

## **What is YARN?**

YARN, or **Yet Another Resource Negotiator**, is the resource management layer in the Hadoop ecosystem. It was introduced in Hadoop 2.x to address the limitations of the original MapReduce framework, where MapReduce acted both as the resource manager and the execution engine. YARN separates the resource management and job scheduling functionalities, which allows multiple applications to run simultaneously in a Hadoop cluster.

In essence, YARN manages resources across the cluster and schedules jobs to run on available nodes, enabling various frameworks (such as MapReduce, Apache Spark, Apache Tez) to run on the same cluster concurrently. It enhances the scalability and flexibility of Hadoop by enabling the execution of a variety of workloads.

## **YARN Architecture**

YARN introduces a two-layer architecture with the following components:

1. **ResourceManager (RM)**:

   * The **ResourceManager** is the master daemon responsible for managing the resources of the cluster. It performs two main functions:

     * **Resource Management**: It tracks available resources (memory, CPU) across all nodes in the cluster.
     * **Job Scheduling**: It assigns resources to different applications based on job priority and resource availability.

   The ResourceManager has two key sub-components:

   * **Scheduler**: The scheduler allocates resources to various running applications based on policies such as capacity, fairness, and priorities.
   * **ApplicationManager**: This component manages the lifecycle of applications, including accepting job submissions and negotiating resources with the ResourceManager.

2. **NodeManager (NM)**:

   * The **NodeManager** runs on each node in the cluster and manages resources on that node.
   * It monitors the resource usage (CPU, memory, disk) on the node and periodically sends resource usage reports to the ResourceManager.
   * The NodeManager also manages the execution of containers (which are isolated environments for running tasks), providing resources like memory and CPU, and ensuring tasks are properly executed on the node.

3. **ApplicationMaster (AM)**:

   * Each application that runs on YARN has its own **ApplicationMaster**.
   * The ApplicationMaster is responsible for negotiating resources with the ResourceManager and managing the lifecycle of tasks (i.e., tasks within MapReduce jobs or other frameworks like Spark).
   * It also monitors the progress of the job and handles task failures or retries.

4. **Containers**:

   * **Containers** are the basic units of execution in YARN. Each container encapsulates a specific amount of resources (e.g., memory, CPU cores) and runs a specific task.
   * The ResourceManager allocates containers to tasks via the NodeManager, which runs them on the node.

## **YARN Job Flow**

Here’s how a typical YARN job is executed:

1. **Job Submission**:

   * A client submits a job to YARN. The job can be a MapReduce, Spark, or any other supported framework job.

2. **Resource Request**:

   * The job’s **ApplicationMaster** communicates with the ResourceManager to request resources (containers) to run the job.

3. **Resource Allocation**:

   * The ResourceManager allocates resources (containers) based on available resources and scheduling policies, then provides these resources to the ApplicationMaster.

4. **Task Execution**:

   * The ApplicationMaster assigns tasks to containers and works with the NodeManagers to start execution.

5. **Task Monitoring**:

   * The NodeManager monitors the status of tasks in the containers and reports to the ResourceManager about the resource usage (CPU, memory).

6. **Completion**:

   * When the job completes, the ApplicationMaster reports back to the ResourceManager and the job is marked as successful or failed based on its progress.

## **ResourceManager Components**

1. **Scheduler**:

   * The Scheduler is responsible for allocating resources to applications based on resource requests. It doesn't track the status of tasks but ensures that applications get resources based on the policies defined.
   * There are different scheduling policies available, such as:

     * **Capacity Scheduler**: Guarantees a certain amount of resources to each application.
     * **Fair Scheduler**: Allocates resources to jobs in a way that all jobs get an equal share of resources over time.

2. **ApplicationManager**:

   * The **ApplicationManager** manages job lifecycle events and acts as the interface for handling the job submission process. It coordinates with the ResourceManager to acquire resources and starts the job in the containers.
   * After receiving the requested resources, the ApplicationMaster initiates tasks within the allocated containers and monitors their execution.

## **NodeManager Components**

1. **Container Management**:

   * The NodeManager’s responsibility is to manage containers, monitor resource usage (such as CPU, memory, disk), and restart tasks if needed.
   * It ensures that tasks are running within their allocated containers and reports resource usage back to the ResourceManager.

2. **Health Checks**:

   * The NodeManager also performs health checks for the tasks running on the node, ensuring that containers are not over-using resources or failing repeatedly.

## **YARN Resource Management Process**

1. **Job Request**: A job is submitted to the YARN ResourceManager.
2. **Container Allocation**: The ResourceManager allocates resources to the job (based on the Scheduler and policies).
3. **Task Execution**: The NodeManager allocates containers on specific nodes for running tasks.
4. **Progress Monitoring**: The NodeManager monitors resource usage and reports to the ResourceManager.
5. **Completion**: When the job finishes, the ApplicationMaster reports the status to the ResourceManager.

## **Advantages of YARN**

1. **Multi-Tenancy**:

   * YARN supports multi-tenancy, which means it allows multiple types of applications (MapReduce, Spark, Tez, etc.) to run on the same Hadoop cluster simultaneously, improving resource utilization and flexibility.

2. **Improved Scalability**:

   * By decoupling resource management from the application logic, YARN allows for better scalability. The ResourceManager can handle larger clusters and manage thousands of nodes and applications.

3. **Fault Tolerance**:

   * YARN’s architecture ensures that jobs can continue running even if individual nodes or applications fail. The ResourceManager and NodeManager handle failure recovery and task rescheduling.

4. **Resource Utilization**:

   * YARN improves resource utilization by managing CPU and memory resources effectively. It ensures that resources are not wasted, and applications use only the resources they need.

5. **Flexibility**:

   * YARN allows different types of data processing frameworks to coexist on the same cluster, making it a more versatile platform for diverse workloads.

## **YARN Configuration Parameters**

Here are some common configuration parameters for YARN:

1. **yarn.resourcemanager.scheduler.class**:

   * Specifies the type of scheduler to be used in YARN (e.g., CapacityScheduler or FairScheduler).

2. **yarn.nodemanager.resource.memory-mb**:

   * Specifies the total amount of memory (in MB) that the NodeManager can allocate to containers on that node.

3. **yarn.nodemanager.resource.cpu-vcores**:

   * Specifies the number of virtual CPU cores that can be allocated to containers on the node.

4. **yarn.scheduler.maximum-allocation-mb**:

   * Specifies the maximum memory allocation for any single container in YARN.

5. **yarn.am.resource.mb**:

   * Defines the memory allocated to the ApplicationMaster.

## **YARN Performance Tuning**

1. **Memory Allocation**:

   * Properly allocating memory to containers is critical for avoiding over-subscription and under-utilization. Set memory limits for containers, and ensure there’s enough memory for the NodeManager to manage the system.

2. **Scheduling Policies**:

   * Choosing the right scheduling policy (Capacity or Fair Scheduler) helps ensure fair resource allocation, especially in a multi-tenant environment with different priority applications.

3. **NodeManager Tuning**:

   * Proper tuning of NodeManager resource monitoring ensures the system is not overburdened. It’s essential to keep the resource usage within limits to avoid task failures and ensure stable performance.
  
### **YARN (Yet Another Resource Negotiator)** Overview and Architecture

YARN is a resource management layer for Hadoop that was introduced in **Hadoop 2.x** to address the limitations of the original MapReduce framework. It separates the resource management and job scheduling functionalities, which were previously handled by the MapReduce framework, into two distinct components: **ResourceManager (RM)** and **NodeManager (NM)**. This architecture enables Hadoop to handle more diverse workloads (beyond just MapReduce), such as **interactive queries**, **streaming data processing**, **machine learning**, and more.

YARN provides a more flexible and efficient way to manage resources, allowing multiple frameworks (like Spark, Tez, and others) to share the same cluster, which increases the cluster's overall utilization.

---

### **YARN Architecture**

The YARN architecture consists of the following main components:

1. **ResourceManager (RM)**:

   * **ResourceManager** is the master daemon that manages the allocation of resources across all applications in the Hadoop cluster. It handles the task of allocating resources (memory and CPU) to the various applications (e.g., MapReduce, Spark, etc.) running on the cluster.
   * The ResourceManager has two main components:

     * **Scheduler**: It allocates resources to the applications based on scheduling policies, such as capacity scheduling or fair scheduling.
     * **ApplicationManager**: It manages the lifecycle of the applications running on YARN, including accepting job submissions and negotiating resources.

2. **NodeManager (NM)**:

   * The **NodeManager** is a slave daemon that runs on each node in the cluster. Its main responsibility is to manage the resources (memory, CPU, etc.) on that specific node.
   * The NodeManager communicates with the ResourceManager and performs tasks such as:

     * Launching containers to execute tasks (e.g., MapReduce, Spark tasks).
     * Monitoring the health of containers and ensuring tasks are completed successfully.
     * Reporting the resource usage (CPU, memory) back to the ResourceManager.

3. **ApplicationMaster (AM)**:

   * The **ApplicationMaster** is a per-application entity responsible for the execution of a specific job in the cluster. It negotiates resources from the ResourceManager and manages the execution of tasks within the containers allocated by NodeManagers.
   * Each application (e.g., a MapReduce job, Spark job) gets its own ApplicationMaster, which ensures the efficient execution of the job.

4. **Containers**:

   * **Containers** are the computational units that run on a node. Each container is assigned a specific amount of resources (CPU, memory) and is managed by the NodeManager. The application runs its tasks in these containers.
   * Containers are isolated from each other, providing resource and memory protection.

---

### **ResourceManager and NodeManager**

#### **ResourceManager (RM)**

The ResourceManager is the central entity responsible for managing the cluster’s resources and scheduling tasks. It has two primary components:

1. **Scheduler**:

   * The **Scheduler** is responsible for allocating resources to various applications based on the cluster’s available resources and the application’s requirements.
   * YARN supports several types of schedulers, including:

     * **CapacityScheduler**: Allows multiple organizations or users to share the cluster by allocating a fixed portion of the cluster's resources.
     * **FairScheduler**: Ensures that all applications get an equal share of the cluster’s resources over time, giving high-priority applications more resources if low-priority applications do not need them.
     * **FIFO Scheduler**: Allocates resources to applications in a First-In-First-Out order, with jobs being processed sequentially.

2. **ApplicationManager**:

   * The **ApplicationManager** is responsible for managing the lifecycle of each application. It starts the **ApplicationMaster** for each job, negotiates the required resources, and monitors job execution.

#### **NodeManager (NM)**

The **NodeManager** runs on each node and is responsible for managing resources and task execution on that node. Each NodeManager manages the resources of the node and is responsible for:

1. **Container Management**:

   * The NodeManager is responsible for launching and monitoring containers where tasks are executed. It ensures that resources are allocated as per the requests made by the ResourceManager.

2. **Resource Reporting**:

   * The NodeManager continuously reports the resource usage (memory, CPU, etc.) of the node to the ResourceManager to assist in resource allocation decisions.

3. **Health Monitoring**:

   * The NodeManager ensures that containers are running as expected. If a task fails or a container becomes unhealthy, it informs the ResourceManager, which can take corrective actions.

---

### **How YARN Manages Resources**

YARN manages resources by allocating available resources to various applications based on the **ResourceManager’s scheduler**. Here’s a breakdown of how resources are managed:

1. **Resource Allocation**:

   * The ResourceManager uses the scheduler to allocate resources (memory and CPU) to applications based on the policies and available cluster resources.
   * Applications specify the amount of resources they need (e.g., memory, CPU cores) when submitting a job.

2. **Resource Requests**:

   * Once a job is submitted, the **ApplicationMaster** of the job negotiates with the ResourceManager for resources. If the resources are available, the ResourceManager allocates the requested resources (containers) on the NodeManager.

3. **Container Assignment**:

   * When resources are allocated, the NodeManager starts containers on the node. These containers are isolated environments where tasks run. The NodeManager assigns these containers to the ApplicationMaster for task execution.

4. **Container Monitoring**:

   * The NodeManager monitors the health of containers and their resource usage (memory, CPU). It also reports resource utilization back to the ResourceManager.
   * If a container is underperforming or the node is overloaded, the ResourceManager may redistribute resources or reassign tasks to different nodes.

5. **Task Execution**:

   * The tasks are executed within containers. The ApplicationMaster coordinates the execution of the tasks and monitors their progress. If a task fails, the ApplicationMaster can request new resources from the ResourceManager and restart the task.

6. **Resource Recycling**:

   * Once tasks are completed, the resources (containers) are released and made available for other applications or tasks in the cluster.

---

### **YARN vs. MapReduce**

#### **MapReduce in Hadoop 1.x (pre-YARN)**

* **MapReduce** in Hadoop 1.x was both the resource manager and the execution framework. It controlled resource allocation (memory, CPU) for each job and ran MapReduce tasks sequentially.
* In this architecture, all jobs were handled by a single **JobTracker** that managed job scheduling and resource management.

#### **YARN in Hadoop 2.x (post-YARN)**

* YARN introduced a **separation of concerns**, allowing **ResourceManager** to handle resource management, while **MapReduce** (or other frameworks like Spark) handles job execution.
* YARN allows for multiple applications (MapReduce, Spark, HBase, etc.) to share the cluster resources, providing better resource utilization and scalability.

#### **Key Differences**:

1. **Resource Management**:

   * **MapReduce**: In the original MapReduce model, the JobTracker handled both job execution and resource management.
   * **YARN**: In YARN, ResourceManager handles resource allocation, and applications (like MapReduce, Spark) run their tasks in isolated containers.

2. **Scalability**:

   * **MapReduce**: Scalability was limited, as MapReduce had a single JobTracker managing all jobs.
   * **YARN**: With YARN, resource management and job execution are separated, enabling better scalability and the ability to run multiple applications on the same cluster.

3. **Multi-tenancy**:

   * **MapReduce**: Only MapReduce jobs could run on a cluster.
   * **YARN**: YARN supports multiple data processing frameworks, enabling more flexible use of the cluster for different workloads (e.g., batch, real-time, and interactive workloads).

4. **Resource Allocation**:

   * **MapReduce**: Fixed allocation of resources (memory and CPU) to MapReduce jobs.
   * **YARN**: More dynamic and flexible allocation of resources to various types of applications based on the scheduler's policies.

---

### **YARN Scheduling and Queues**

YARN supports different scheduling algorithms to efficiently allocate resources to applications. The key components involved in YARN scheduling are:

1. **Scheduler**:

   * The **Scheduler** is responsible for allocating resources to different applications based on a defined policy. It doesn’t monitor job execution or handle failure recovery.
   * YARN supports different types of schedulers:

     * **CapacityScheduler**: Designed for multi-tenant clusters where multiple organizations or teams share a cluster. It assigns a portion of cluster resources to each organization or team.
     * **FairScheduler**: Ensures fair allocation of resources across jobs, giving each job a fair share of cluster resources over time. It is ideal for clusters where multiple users run jobs simultaneously.
     * **FIFO Scheduler**: Resources are allocated to jobs in a First In, First Out (FIFO) order, where the first submitted job gets resources first.

2. **Queues**:

   * YARN allows jobs to be submitted to different **queues**, and the **Scheduler** manages resource allocation based on these queues.
   * Queues can be defined with certain resource limits, priorities, and policies to ensure that certain types of workloads or users receive higher or lower priority in resource allocation.
   * Example of a capacity scheduling policy:

     * Team A might get 50% of the cluster’s resources, while Team B gets 30%, and the remaining 20% is used for overflow or other workloads.

   Queues are managed and configured in `capacity-scheduler.xml`, where you define the different queues and the resources they should be allocated.

#### **Example of YARN Scheduler Configuration**:

```xml
<configuration>
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>queueA,queueB</value>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.queueA.capacity</name>
    <value>50</value>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.queueB.capacity</name>
    <value>50</value>
  </property>
</configuration>
```

In this example:

* **queueA** gets 50% of the available cluster resources.
* **queueB** gets the remaining 50%.

---

### **Conclusion**

YARN is a powerful resource management layer that improves the scalability, flexibility, and efficiency of Hadoop clusters. It allows different data processing frameworks to run simultaneously on the same cluster, with dynamic resource allocation and management. The separation of resource management (handled by YARN) from job execution (handled by MapReduce, Spark, etc.) helps in optimizing resource utilization and managing different workloads effectively. Understanding how YARN works, its architecture, and its scheduling mechanisms is crucial for building efficient, scalable data processing pipelines in a Hadoop environment.


## **Conclusion**

YARN is a crucial component of the Hadoop ecosystem, providing scalable and efficient resource management for a wide range of data processing applications. By separating resource management from job execution, it enables better resource utilization, job prioritization, and scalability. Whether you’re running MapReduce, Apache Spark, or other frameworks, YARN plays a vital role in ensuring the success of your Hadoop-based big data workflows.
